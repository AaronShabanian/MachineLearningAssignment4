{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15860d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import wget\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, Activation, Embedding, Bidirectional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af23d0f2",
   "metadata": {},
   "source": [
    "Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f88db1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-03-31 17:39:30--  https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.72.144, 142.250.72.176, 142.251.40.48, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.72.144|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5057493 (4.8M) [text/csv]\n",
      "Saving to: 'C:/Users/a2sha/Documents/MachineLearning/MachineLearningAssignment4/bbc-text.csv'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  1% 2.30M 2s\n",
      "    50K .......... .......... .......... .......... ..........  2% 4.65M 2s\n",
      "   100K .......... .......... .......... .......... ..........  3% 4.46M 1s\n",
      "   150K .......... .......... .......... .......... ..........  4% 2.54M 1s\n",
      "   200K .......... .......... .......... .......... ..........  5% 4.49M 1s\n",
      "   250K .......... .......... .......... .......... ..........  6% 4.04M 1s\n",
      "   300K .......... .......... .......... .......... ..........  7% 5.39M 1s\n",
      "   350K .......... .......... .......... .......... ..........  8% 4.73M 1s\n",
      "   400K .......... .......... .......... .......... ..........  9% 6.15M 1s\n",
      "   450K .......... .......... .......... .......... .......... 10% 5.59M 1s\n",
      "   500K .......... .......... .......... .......... .......... 11% 4.25M 1s\n",
      "   550K .......... .......... .......... .......... .......... 12% 6.64M 1s\n",
      "   600K .......... .......... .......... .......... .......... 13% 7.43M 1s\n",
      "   650K .......... .......... .......... .......... .......... 14% 6.96M 1s\n",
      "   700K .......... .......... .......... .......... .......... 15% 7.41M 1s\n",
      "   750K .......... .......... .......... .......... .......... 16% 5.81M 1s\n",
      "   800K .......... .......... .......... .......... .......... 17% 6.78M 1s\n",
      "   850K .......... .......... .......... .......... .......... 18% 6.31M 1s\n",
      "   900K .......... .......... .......... .......... .......... 19% 7.41M 1s\n",
      "   950K .......... .......... .......... .......... .......... 20% 6.55M 1s\n",
      "  1000K .......... .......... .......... .......... .......... 21% 6.80M 1s\n",
      "  1050K .......... .......... .......... .......... .......... 22% 6.35M 1s\n",
      "  1100K .......... .......... .......... .......... .......... 23% 7.01M 1s\n",
      "  1150K .......... .......... .......... .......... .......... 24% 4.93M 1s\n",
      "  1200K .......... .......... .......... .......... .......... 25% 6.03M 1s\n",
      "  1250K .......... .......... .......... .......... .......... 26% 4.74M 1s\n",
      "  1300K .......... .......... .......... .......... .......... 27% 6.90M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 28% 9.32M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 29% 6.02M 1s\n",
      "  1450K .......... .......... .......... .......... .......... 30% 9.37M 1s\n",
      "  1500K .......... .......... .......... .......... .......... 31% 4.83M 1s\n",
      "  1550K .......... .......... .......... .......... .......... 32% 5.52M 1s\n",
      "  1600K .......... .......... .......... .......... .......... 33% 9.78M 1s\n",
      "  1650K .......... .......... .......... .......... .......... 34% 7.26M 1s\n",
      "  1700K .......... .......... .......... .......... .......... 35%  782K 1s\n",
      "  1750K .......... .......... .......... .......... .......... 36% 9.10M 1s\n",
      "  1800K .......... .......... .......... .......... .......... 37% 7.12M 1s"
     ]
    }
   ],
   "source": [
    "\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv \\\n",
    "    -O C:\\Users\\a2sha\\Documents\\MachineLearning\\MachineLearningAssignment4\\bbc-text.csv\n",
    "    #-O /tmp/bbc-text.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f554218d",
   "metadata": {},
   "source": [
    "Setting up nltk and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f6220fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1850K .......... .......... .......... .......... .......... 38% 11.7M 1s\n",
      "  1900K .......... .......... .......... .......... .......... 39% 8.74M 1s\n",
      "  1950K .......... .......... .......... .......... .......... 40% 5.92M 1s\n",
      "  2000K .......... .......... .......... .......... .......... 41% 8.08M 1s\n",
      "  2050K .......... .......... .......... .......... .......... 42% 9.61M 1s\n",
      "  2100K .......... .......... .......... .......... .......... 43% 7.08M 1s\n",
      "  2150K .......... .......... .......... .......... .......... 44% 10.3M 1s\n",
      "  2200K .......... .......... .......... .......... .......... 45% 4.84M 1s\n",
      "  2250K .......... .......... .......... .......... .......... 46% 5.82M 1s\n",
      "  2300K .......... .......... .......... .......... .......... 47% 4.58M 0s\n",
      "  2350K .......... .......... .......... .......... .......... 48% 3.39M 0s\n",
      "  2400K .......... .......... .......... .......... .......... 49% 3.91M 0s\n",
      "  2450K .......... .......... .......... .......... .......... 50% 2.72M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 51% 4.59M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 52% 13.1M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 53% 8.12M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 54% 6.64M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 55% 10.8M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 56% 2.79M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 57% 9.01M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 58% 8.65M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 59% 9.04M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 60% 10.3M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 61% 11.3M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 62% 10.5M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 63% 8.16M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 64% 4.82M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 65% 10.9M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 66% 6.56M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 67% 6.80M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 68% 9.97M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 69% 10.7M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 70% 3.56M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 71% 5.75M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 72% 6.26M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 73% 5.64M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 74% 7.04M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 75% 7.82M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 76% 8.77M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 77% 8.29M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 78% 6.03M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 79% 7.29M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 80% 8.08M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 82% 4.61M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 83% 8.42M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 84% 10.1M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 85% 7.29M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 86% 6.18M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 87% 9.53M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 88% 18.6M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 89% 6.10M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 90% 14.6M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 91% 8.89M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 92% 12.3M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 93% 4.88M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 94% 9.40M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 95% 4.61M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 96% 5.93M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 97% 13.4M 0s\n",
      "  4800K .......... .......... .......... .......... .......... 98% 9.65M 0s\n",
      "  4850K .......... .......... .......... .......... .......... 99% 10.7M 0s\n",
      "  4900K .......... .......... .......... ........             100% 14.2M=0.8s\n",
      "\n",
      "2022-03-31 17:39:31 (5.90 MB/s) - 'C:/Users/a2sha/Documents/MachineLearning/MachineLearningAssignment4/bbc-text.csv' saved [5057493/5057493]\n",
      "\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\a2sha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7640f9a6",
   "metadata": {},
   "source": [
    "Set Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9980a2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000 # make the top list of words (common words)\n",
    "embedding_dim = 64\n",
    "max_length = 200\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>' # OOV = Out of Vocabulary\n",
    "training_portion = .8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09cae53",
   "metadata": {},
   "source": [
    "Populate the list of articles and labels from the data and also remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8284ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "labels = []\n",
    "\n",
    "with open(\"C:/Users/a2sha/Documents/MachineLearning/MachineLearningAssignment4/bbc-text.csv\", 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        labels.append(row[0])\n",
    "        article = row[1]\n",
    "        for word in STOPWORDS:\n",
    "            token = ' ' + word + ' '\n",
    "            article = article.replace(token, ' ')\n",
    "            article = article.replace(' ', ' ')\n",
    "        articles.append(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86abdf88",
   "metadata": {},
   "source": [
    "Creating train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eacbcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(articles) * training_portion)\n",
    "\n",
    "train_articles = articles[0: train_size]\n",
    "train_labels = labels[0: train_size]\n",
    "\n",
    "validation_articles = articles[train_size:]\n",
    "validation_labels = labels[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bb1afe",
   "metadata": {},
   "source": [
    "Tokenize and Sequenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dd5d5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_articles)\n",
    "word_index = tokenizer.word_index\n",
    "train_sequences = tokenizer.texts_to_sequences(train_articles)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "validation_sequences = tokenizer.texts_to_sequences(validation_articles)\n",
    "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b723ea5",
   "metadata": {},
   "source": [
    "Setting up labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22811d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(labels)\n",
    "\n",
    "training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
    "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ab134a",
   "metadata": {},
   "source": [
    "Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "183b54fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 64)          320000    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 64)          0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              66048     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 386,822\n",
      "Trainable params: 386,822\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Bidirectional(LSTM(embedding_dim)))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980fbae7",
   "metadata": {},
   "source": [
    "Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50a72251",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62c62cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "56/56 - 26s - loss: 1.5919 - accuracy: 0.2899 - val_loss: 1.5724 - val_accuracy: 0.2697 - 26s/epoch - 457ms/step\n",
      "Epoch 2/10\n",
      "56/56 - 12s - loss: 1.1817 - accuracy: 0.5511 - val_loss: 0.7590 - val_accuracy: 0.8404 - 12s/epoch - 221ms/step\n",
      "Epoch 3/10\n",
      "56/56 - 13s - loss: 0.6072 - accuracy: 0.8236 - val_loss: 0.6074 - val_accuracy: 0.7978 - 13s/epoch - 225ms/step\n",
      "Epoch 4/10\n",
      "56/56 - 12s - loss: 0.3499 - accuracy: 0.9292 - val_loss: 0.4261 - val_accuracy: 0.8854 - 12s/epoch - 217ms/step\n",
      "Epoch 5/10\n",
      "56/56 - 13s - loss: 0.1963 - accuracy: 0.9483 - val_loss: 0.3662 - val_accuracy: 0.9011 - 13s/epoch - 225ms/step\n",
      "Epoch 6/10\n",
      "56/56 - 13s - loss: 0.1453 - accuracy: 0.9764 - val_loss: 0.2600 - val_accuracy: 0.9281 - 13s/epoch - 226ms/step\n",
      "Epoch 7/10\n",
      "56/56 - 12s - loss: 0.0547 - accuracy: 0.9955 - val_loss: 0.1976 - val_accuracy: 0.9461 - 12s/epoch - 211ms/step\n",
      "Epoch 8/10\n",
      "56/56 - 11s - loss: 0.0208 - accuracy: 0.9978 - val_loss: 0.2015 - val_accuracy: 0.9416 - 11s/epoch - 197ms/step\n",
      "Epoch 9/10\n",
      "56/56 - 11s - loss: 0.0184 - accuracy: 0.9989 - val_loss: 0.1424 - val_accuracy: 0.9551 - 11s/epoch - 199ms/step\n",
      "Epoch 10/10\n",
      "56/56 - 12s - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.1723 - val_accuracy: 0.9483 - 12s/epoch - 223ms/step\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(train_padded, training_label_seq, epochs=num_epochs, \n",
    "                    validation_data=(validation_padded, validation_label_seq), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c529dcf",
   "metadata": {},
   "source": [
    "Prediction Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c47975b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.5735817e-04 2.6085167e-04 3.2820776e-03 9.9299192e-01 1.1172922e-03\n",
      "  2.1904306e-03]]\n",
      "3\n",
      "politics\n"
     ]
    }
   ],
   "source": [
    "\n",
    "txt = [\"blair prepares to name poll date tony blair is likely to name 5 may as election day when parliament returns from its easter break  the bbc s political editor has learned.  andrew marr says mr blair will ask the queen on 4 or 5 april to dissolve parliament at the end of that week. mr blair has so far resisted calls for him to name the day but all parties have stepped up campaigning recently. downing street would not be drawn on the claim  saying election timing was a matter for the prime minister.  a number 10 spokeswoman would only say:  he will announce an election when he wants to announce an election.  the move will signal a frantic week at westminster as the government is likely to try to get key legislation through parliament. the government needs its finance bill  covering the budget plans  to be passed before the commons closes for business at the end of the session on 7 april.  but it will also seek to push through its serious and organised crime bill and id cards bill. mr marr said on wednesday s today programme:  there s almost nobody at a senior level inside the government or in parliament itself who doesn t expect the election to be called on 4 or 5 april.  as soon as the commons is back after the short easter recess  tony blair whips up to the palace  asks the queen to dissolve parliament ... and we re going.  the labour government officially has until june 2006 to hold general election  but in recent years governments have favoured four-year terms.\"]\n",
    "\n",
    "seq = tokenizer.texts_to_sequences(txt)\n",
    "padded = pad_sequences(seq, maxlen=max_length)\n",
    "pred = model.predict(padded)\n",
    "labels = ['sport', 'bussiness', 'politics', 'tech', 'entertainment'] \n",
    "\n",
    "print(pred)\n",
    "print(np.argmax(pred))\n",
    "print(labels[np.argmax(pred)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dfababb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.1402832e-04 2.4513642e-03 9.7615951e-01 2.0509444e-02 9.6237731e-05\n",
      "  6.6940807e-04]]\n",
      "2\n",
      "bussiness\n"
     ]
    }
   ],
   "source": [
    "\n",
    "txt = [\"call to save manufacturing jobs the trades union congress (tuc) is calling on the government to stem job losses in manufacturing firms by reviewing the help it gives companies.  the tuc said in its submission before the budget that action is needed because of 105 000 jobs lost from the sector over the last year. it calls for better pensions  child care provision and decent wages. the 36-page submission also urges the government to examine support other european countries provide to industry. tuc general secretary brendan barber called for  a commitment to policies that will make a real difference to the lives of working people.    greater investment in childcare strategies and the people delivering that childcare will increases the options available to working parents   he said.  a commitment to our public services and manufacturing sector ensures that we can continue to compete on a global level and deliver the frontline services that this country needs.  he also called for  practical measures  to help pensioners  especially women who he said  are most likely to retire in poverty . the submission also calls for decent wages and training for people working in the manufacturing sector.\"]\n",
    "\n",
    "seq = tokenizer.texts_to_sequences(txt)\n",
    "padded = pad_sequences(seq, maxlen=max_length)\n",
    "pred = model.predict(padded)\n",
    "labels = ['sport', 'bussiness', 'politics', 'tech', 'entertainment'] \n",
    "\n",
    "print(pred)\n",
    "print(np.argmax(pred))\n",
    "print(labels[np.argmax(pred)-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17658497",
   "metadata": {},
   "source": [
    "I can see that it correctly predicted them as politics and business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe344e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
